{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Домашнее задание 4\n",
        "\n",
        "В домашнем задании нужно обучить модель для ответов на вопросы. Будем использовать датасет SQUAD, включающий вопросы, контекст и ответы. \n",
        "\n",
        "Цель задания: поэкспериментировать с генерацией и проанализировать результаты. В качестве модели можно выбрать модель на основе декодера трансформера или модель с архитектурой Encoder-Decoder.\n",
        "\n",
        "Баллы за ДЗ:\n",
        "\n",
        "\n",
        "\n",
        "*   Предобработка и токенизатор - 2 балл\n",
        "*   Загрузка и обучение модели - 3 балла\n",
        "*   Инференс и эксперименты - 3 балла\n",
        "*   Отчёт - 2 балла\n",
        "* Бонус - 5 баллов\n"
      ],
      "metadata": {
        "id": "nbdvRZ7jd3OL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "foU7fMs-3zjc",
        "outputId": "dbea0bdc-9904-4c68-edf4-1aae64e264ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Загузка датасета"
      ],
      "metadata": {
        "id": "6qKRmlA81ata"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers datasets adapter-transformers --quiet"
      ],
      "metadata": {
        "id": "FTQMN3Ai4w5e"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from transformers import Pipeline, AutoTokenizer, TrainingArguments, Trainer,  DataCollatorForLanguageModeling,  AutoAdapterModel\n",
        "import torch"
      ],
      "metadata": {
        "id": "kIAdfDLs5Zac"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds = load_dataset('squad')"
      ],
      "metadata": {
        "id": "jAj2mhQf5gwL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds"
      ],
      "metadata": {
        "id": "XrUPYXid6V9t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ds['train'][0]"
      ],
      "metadata": {
        "id": "_N-yBQEK77C8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Предобработка и токенизатор"
      ],
      "metadata": {
        "id": "aJNyPJm51fD5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Далее нужно подготовить и токенизировать данные для обучения модели. \n",
        "\n",
        "В зависимости от выбора модели формат представления данных будет различаться.\n",
        "\n",
        "* Для генеративной модели на основе декодера (GPT) нужно подготовить промпт в качестве условия для генерации. Например, данные для одного вопроса могут выглядеть так: '<context> Question: <question> Answer: <answer>'. С форматом промпта можно экспеиментировать.\n",
        "\n",
        "* Для Encoder-Decoder модели (например, T5) нужно отдельно токенизировать входные данные (контекст и вопрос) и таргет (ответ). В начале инпута возможно нужно будет указать префикс задания для модели.\n",
        "\n"
      ],
      "metadata": {
        "id": "eutYQlrdwREI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "c_train = ds['train']['context'] \n",
        "q_train = ds['train']['question']\n",
        "a_train = [i['text'][0] for i in ds['train']['answers']]\n",
        "c_val = ds['validation']['context']\n",
        "q_val = ds['validation']['question']\n",
        "a_val = [i['text'][0] for i in ds['validation']['answers']]"
      ],
      "metadata": {
        "id": "r7LdzmXfGW8w"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# соединяем контекст и вопрос\n",
        "cq_train = [c_train[i] + ' @QUESTION@ ' + q_train[i] for i in range(len(c_train))]\n",
        "cq_val = [c_val[i] + ' @QUESTION@ ' + q_val[i] for i in range(len(c_val))]"
      ],
      "metadata": {
        "id": "fI7tlZuR6lci"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PairsDataset(Dataset):\n",
        "    def __init__(self, x, y):\n",
        "        self.x = x\n",
        "        self.y = y\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        assert idx <= len(self.x['input_ids']), (idx, len(self.x['input_ids']))\n",
        "        item = {key: val[idx] for key, val in self.x.items()}\n",
        "        item['decoder_attention_mask'] = self.y['attention_mask'][idx]\n",
        "        item['labels'] = self.y['input_ids'][idx]\n",
        "        return item\n",
        "    \n",
        "    @property\n",
        "    def n(self):\n",
        "        return len(self.x['input_ids'])\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.n"
      ],
      "metadata": {
        "id": "PBXnMrzzwFTu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained('t5-base')"
      ],
      "metadata": {
        "id": "BOHxnCaR8z6r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PairsDataset(tokenizer(cq_train), tokenizer(a_train))\n",
        "test_dataset = PairsDataset(tokenizer(cq_val), tokenizer(a_val))"
      ],
      "metadata": {
        "id": "2EXsURWZ88vj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "На этом моменте таймлайна я успела три раза словить куда аут оф мемори , поэтому let me introduce some костыли:"
      ],
      "metadata": {
        "id": "mIDpU867Ccin"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# посмотрим на распределение длин примеров в трейне\n",
        "lengths = [len(train_dataset[i]['input_ids']) for i in range(len(train_dataset))]\n",
        "df = pd.DataFrame(lengths, columns = ['lns'])\n",
        "df.describe()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 300
        },
        "id": "iET6KwWACFay",
        "outputId": "3abfc1c8-8118-4dbb-85a9-1333ab7ad496"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                lns\n",
              "count  87599.000000\n",
              "mean     194.590178\n",
              "std       74.628638\n",
              "min       40.000000\n",
              "25%      147.000000\n",
              "50%      180.000000\n",
              "75%      228.000000\n",
              "max      987.000000"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-ca674ddf-b7b8-4452-a0ab-d8dc33de3b5e\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>lns</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>87599.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>194.590178</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>74.628638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>40.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>147.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>180.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>228.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>987.000000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ca674ddf-b7b8-4452-a0ab-d8dc33de3b5e')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-ca674ddf-b7b8-4452-a0ab-d8dc33de3b5e button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-ca674ddf-b7b8-4452-a0ab-d8dc33de3b5e');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "75% данных не превышают 228 токенов, поэтому я удалю из трейна все, что длиннее 256. Я не обрезаю до 256 токенов в токенизаторе, потому что в этом случае у части примеров в инпут просто не попадет сам вопрос, а это лишний шум в трейне"
      ],
      "metadata": {
        "id": "pYRsuAPFCT1g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(валидационную выборку трогать не стала, после обучения можно будет инференситься на последовательностях > 256)\n"
      ],
      "metadata": {
        "id": "mKWiEeEUEgp1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "new_cq_train = []\n",
        "new_a_train = []\n",
        "\n",
        "for i in range(len(cq_train)):\n",
        "  if len(train_dataset[i]['input_ids']) <= 256:\n",
        "    new_cq_train.append(cq_train[i])\n",
        "    new_a_train.append(a_train[i])"
      ],
      "metadata": {
        "id": "KRKS8O_yCY9l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(cq_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v5y-jPYDDu1l",
        "outputId": "6019d96e-7919-4845-e12a-2d7ce38cbb54"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "87599"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(new_cq_train) "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rN34FFKXDpIW",
        "outputId": "d1a70c39-8fed-4c3b-c1e6-d40453f67887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "73076"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataset = PairsDataset(tokenizer(new_cq_train), tokenizer(new_a_train))\n",
        "test_dataset = PairsDataset(tokenizer(cq_val), tokenizer(a_val))"
      ],
      "metadata": {
        "id": "nX-gDi3_Dhwt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Обучение модели"
      ],
      "metadata": {
        "id": "RylKEE9g5pUd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import List, Dict, Union\n",
        "\n",
        "class DataCollatorWithPadding:\n",
        "    def __init__(self, tokenizer):\n",
        "        self.tokenizer = tokenizer\n",
        "\n",
        "    def __call__(self, features: List[Dict[str, Union[List[int], torch.Tensor]]]) -> Dict[str, torch.Tensor]:\n",
        "        batch = self.tokenizer.pad(\n",
        "            features,\n",
        "            padding=True,\n",
        "        )\n",
        "        ybatch = self.tokenizer.pad(\n",
        "            {'input_ids': batch['labels'], 'attention_mask': batch['decoder_attention_mask']},\n",
        "            padding=True,\n",
        "        ) \n",
        "        batch['labels'] = ybatch['input_ids']\n",
        "        batch['decoder_attention_mask'] = ybatch['attention_mask']\n",
        "        \n",
        "        return {k: torch.tensor(v) for k, v in batch.items()}"
      ],
      "metadata": {
        "id": "_RbWQxBO8cwy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoAdapterModel.from_pretrained('t5-base')\n",
        "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
      ],
      "metadata": {
        "id": "e-sBu2lm9TcD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.add_seq2seq_lm_head('qua_head')\n",
        "model.add_adapter('qua_adapter')\n",
        "model.train_adapter('qua_adapter')"
      ],
      "metadata": {
        "id": "aSpBVIDisQPH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(output_dir='/folder',\n",
        "                                  per_device_train_batch_size = 32,\n",
        "                                  per_device_eval_batch_size = 32,\n",
        "                                  save_strategy = 'no',\n",
        "                                  num_train_epochs=1,\n",
        "                                  load_best_model_at_end = False,\n",
        "                                  evaluation_strategy = 'epoch')"
      ],
      "metadata": {
        "id": "iqBUx_-iAhb7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "wheuKfi-iwRq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model = model,\n",
        "    args = training_args,\n",
        "    train_dataset = train_dataset,\n",
        "    eval_dataset = test_dataset,\n",
        "    tokenizer = tokenizer,\n",
        "    data_collator = data_collator)"
      ],
      "metadata": {
        "id": "cOXP465NAwHJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()\n",
        "model.save_adapter('/content/drive/MyDrive/NNLP_HW4/t5_qua_adapter_long', 'qua_adapter')"
      ],
      "metadata": {
        "id": "FMijhpxjA0NJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Инференс модели"
      ],
      "metadata": {
        "id": "S9hd5Jt38mL0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Запустите модель на нескольких примерах с помощью пайплайна. Можно поэкспериментировать с декодированием, задать разные параметры генерации. Сравните результаты."
      ],
      "metadata": {
        "id": "HcpLhhOy9FTT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model = AutoAdapterModel.from_pretrained('t5-base')\n",
        "tokenizer = AutoTokenizer.from_pretrained('t5-base')"
      ],
      "metadata": {
        "id": "I6RWDGzK9zhP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "adapter_name = model.load_adapter(\"/content/drive/MyDrive/NNLP_HW4/t5_qua_adapter_long\")\n",
        "model.add_seq2seq_lm_head('qua_head')\n",
        "model.set_active_adapters(adapter_name)\n",
        "model.to('cuda')"
      ],
      "metadata": {
        "id": "uIV9Il1bQ1gc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class QuAnswering1(Pipeline):\n",
        "    def _sanitize_parameters(self, **kwargs):\n",
        "      preprocess_kwargs = {}\n",
        "      if \"second_text\" in kwargs:\n",
        "          preprocess_kwargs[\"second_text\"] = kwargs[\"second_text\"]\n",
        "      return preprocess_kwargs, {}, {}\n",
        "\n",
        "    def preprocess(self, text, second_text=None):\n",
        "      return self.tokenizer.encode(text, text_pair=second_text, return_tensors=self.framework).to('cuda')\n",
        "\n",
        "    def _forward(self, model_inputs):\n",
        "      model_outputs = self.model.generate(model_inputs, max_length = 30, \n",
        "                           do_sample=False)\n",
        "      return model_outputs\n",
        "\n",
        "    def postprocess(self, model_outputs):\n",
        "      return self.tokenizer.decode(model_outputs[0], skip_special_tokens=True)\n",
        "    "
      ],
      "metadata": {
        "id": "LNLBuppnRZ9u"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pipeline1 = QuAnswering1(model=model, tokenizer=tokenizer, framework='pt', device=0)"
      ],
      "metadata": {
        "id": "Tu_A-AsCS9Dh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num, v in enumerate(cq_val[:10]):\n",
        "  print(cq_val[num].split('@')[-1])\n",
        "  answer = pipeline1((v))\n",
        "  print(f'PREDICTED ANSWER: {answer}')\n",
        "  print(f'TRUE ANSWER: {a_val[num]}')\n",
        "  print('-'*15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J29RHd2jWfnS",
        "outputId": "76ff9913-fbb0-4179-d56b-b15e5707a7a5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Which NFL team represented the AFC at Super Bowl 50?\n",
            "PREDICTED ANSWER: Denver Broncos\n",
            "TRUE ANSWER: Denver Broncos\n",
            "---------------\n",
            " Which NFL team represented the NFC at Super Bowl 50?\n",
            "PREDICTED ANSWER: Carolina Panthers\n",
            "TRUE ANSWER: Carolina Panthers\n",
            "---------------\n",
            " Where did Super Bowl 50 take place?\n",
            "PREDICTED ANSWER: Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "TRUE ANSWER: Santa Clara, California\n",
            "---------------\n",
            " Which NFL team won Super Bowl 50?\n",
            "PREDICTED ANSWER: Denver Broncos\n",
            "TRUE ANSWER: Denver Broncos\n",
            "---------------\n",
            " What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
            "PREDICTED ANSWER: gold\n",
            "TRUE ANSWER: gold\n",
            "---------------\n",
            " What was the theme of Super Bowl 50?\n",
            "PREDICTED ANSWER: gold\n",
            "TRUE ANSWER: \"golden anniversary\"\n",
            "---------------\n",
            " What day was the game played on?\n",
            "PREDICTED ANSWER: February 7, 2016\n",
            "TRUE ANSWER: February 7, 2016\n",
            "---------------\n",
            " What is the AFC short for?\n",
            "PREDICTED ANSWER: American Football Conference\n",
            "TRUE ANSWER: American Football Conference\n",
            "---------------\n",
            " What was the theme of Super Bowl 50?\n",
            "PREDICTED ANSWER: gold\n",
            "TRUE ANSWER: \"golden anniversary\"\n",
            "---------------\n",
            " What does AFC stand for?\n",
            "PREDICTED ANSWER: American Football Conference\n",
            "TRUE ANSWER: American Football Conference\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# попробуем генерировать несколько вариантов с top-k sampling\n",
        "\n",
        "class QuAnswering2(Pipeline):\n",
        "    def _sanitize_parameters(self, **kwargs):\n",
        "      preprocess_kwargs = {}\n",
        "      if \"second_text\" in kwargs:\n",
        "          preprocess_kwargs[\"second_text\"] = kwargs[\"second_text\"]\n",
        "      return preprocess_kwargs, {}, {}\n",
        "\n",
        "    def preprocess(self, text, second_text=None):\n",
        "      return self.tokenizer.encode(text, text_pair=second_text, return_tensors=self.framework).to('cuda')\n",
        "\n",
        "    def _forward(self, model_inputs):\n",
        "      model_outputs = self.model.generate(model_inputs, max_length = 30, \n",
        "                           do_sample=True, top_k=20, temperature = 0.8,\n",
        "                           num_return_sequences=5)\n",
        "      return model_outputs\n",
        "\n",
        "    def postprocess(self, model_outputs):\n",
        "      otps = [self.tokenizer.decode(i, skip_special_tokens=True) for i in model_outputs]\n",
        "      return otps\n",
        "\n",
        "pipeline2 = QuAnswering2(model=model, tokenizer=tokenizer, framework='pt', device=0)"
      ],
      "metadata": {
        "id": "TSj0poYCYKvy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for num, v in enumerate(cq_val[:10]):\n",
        "  print(cq_val[num].split('@')[-1]+'\\n')\n",
        "  answer = '\\n'.join(pipeline2((v)))\n",
        "  print(f'PREDICTED ANSWERS:\\n{answer}')\n",
        "  print(f'TRUE ANSWER: {a_val[num]}')\n",
        "  print('-'*15)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "io2Ngs1sY4WC",
        "outputId": "fabddde9-9abc-4b6f-e496-f017f3c33413"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Which NFL team represented the AFC at Super Bowl 50?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "TRUE ANSWER: Denver Broncos\n",
            "---------------\n",
            " Which NFL team represented the NFC at Super Bowl 50?\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1048: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "PREDICTED ANSWERS:\n",
            "Carolina Panthers\n",
            "Carolina Panthers\n",
            "Carolina Panthers\n",
            "Carolina Panthers\n",
            "Carolina Panthers\n",
            "TRUE ANSWER: Carolina Panthers\n",
            "---------------\n",
            " Where did Super Bowl 50 take place?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "Levi's Stadium in the San Francisco Bay Area\n",
            "Levi's Stadium in the San Francisco Bay Area at Santa Clara, California\n",
            "Levi's Stadium\n",
            "TRUE ANSWER: Santa Clara, California\n",
            "---------------\n",
            " Which NFL team won Super Bowl 50?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "Denver Broncos\n",
            "TRUE ANSWER: Denver Broncos\n",
            "---------------\n",
            " What color was used to emphasize the 50th anniversary of the Super Bowl?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "gold\n",
            "gold\n",
            "gold\n",
            "gold\n",
            "gold\n",
            "TRUE ANSWER: gold\n",
            "---------------\n",
            " What was the theme of Super Bowl 50?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "gold\n",
            "golden anniversary\n",
            "gold\n",
            "gold\n",
            "gold\n",
            "TRUE ANSWER: \"golden anniversary\"\n",
            "---------------\n",
            " What day was the game played on?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "February 7, 2016\n",
            "February 7, 2016\n",
            "February 7, 2016\n",
            "February 7, 2016\n",
            "February 7, 2016\n",
            "TRUE ANSWER: February 7, 2016\n",
            "---------------\n",
            " What is the AFC short for?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "TRUE ANSWER: American Football Conference\n",
            "---------------\n",
            " What was the theme of Super Bowl 50?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "gold\n",
            "gold\n",
            "gold\n",
            "gold\n",
            "golden anniversary\n",
            "TRUE ANSWER: \"golden anniversary\"\n",
            "---------------\n",
            " What does AFC stand for?\n",
            "\n",
            "PREDICTED ANSWERS:\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "American Football Conference\n",
            "TRUE ANSWER: American Football Conference\n",
            "---------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Очень грубое и не очень честное accuracy на микро-выборке:\n",
        "acc1 = 0\n",
        "acc2 = 0\n",
        "for i in range(100):\n",
        "  a1 = pipeline1((cq_val[i]))\n",
        "  a2 = pipeline1((cq_val[i]))\n",
        "  y = a_val[i]\n",
        "  if a1 == y: # строгое равенство предикта и таргета при жадном поиске\n",
        "    acc1 += 1\n",
        "  if y in a2: # наличие таргета среди предиктов при top-k sampling\n",
        "    acc2 += 1  \n",
        "\n",
        "print(f'acc1: {acc1/100}')\n",
        "print(f'acc2: {acc2/100}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UvXoBTZZdv0J",
        "outputId": "04261e2f-a1e4-4b3d-fcd8-6cd9266fafb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/transformers/pipelines/base.py:1048: UserWarning: You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "acc1: 0.77\n",
            "acc2: 0.82\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Отчет\n",
        "Изначальный план был воспользоваться тем фактом, что т5 принимает неограниченную длину контекста и дообучить под нее адаптер в целях экономии вычислительных ресурсов. Но у меня не получилось вписаться на гпу с таким сеттингом, поэтому я оставила только примеры <= 256 токенов (но адаптер тоже оставила, потому что он по перфомансу сопоставим с файн-тюном, а иногда даже лучше, а памяти на гпу и диске требует меньше)\n",
        "\n",
        "В целом по тем примерaм, на которые я посмотрела, модель справляется хорошо (~0.8 accuracy). Так как ответ содержится в тексте и таск в целом не очень креативный, top-k sampling не сильно меняет картину, но скорее полезен, чем нет (\"golden anniversary\") + c некоторыми ответами модели я согласна больше, чем с таргетом (полная локация места супербоула)\n",
        "\n",
        "\n",
        "    "
      ],
      "metadata": {
        "id": "mw0V8aWd-HDT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Бонус"
      ],
      "metadata": {
        "id": "CaCxHN6p_Ja7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Я вынесла это в [отдельную тетрадку](https://colab.research.google.com/drive/1Cew3cVcWMtB2lF2U9LE13vBnynsU7hQd?usp=sharing)"
      ],
      "metadata": {
        "id": "JqXp4fpB2g6L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "В датасете SQUAD ответы на вопросы содержатся внутри контекста. Можно попробовать обучить модель на основе энкодера для предсказания начала и окончания ответа по токенам контекста:\n",
        "![](https://media.springernature.com/lw685/springer-static/image/chp%3A10.1007%2F978-1-4842-6664-9_5/MediaObjects/498208_1_En_5_Fig2_HTML.jpg)"
      ],
      "metadata": {
        "id": "JTVupTBY_xtD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eXhOzYH6-MW1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}